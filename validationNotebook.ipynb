{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we read the data in memory. The file uses ; as a delimiter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read training data\n",
    "\n",
    "train_data = pd.read_csv('train.csv', delimiter=';')\n",
    "#train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data before cleaning it to avoid data leakage for sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(train_data, test_size=0.4, random_state=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just checking that we have a rational and representative split of possitive cases in both splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "171\n",
      "137\n"
     ]
    }
   ],
   "source": [
    "print((train_data.fraud == \"Y\").sum())\n",
    "print((test_data.fraud == \"Y\").sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean variables starting with claim_\n",
    "\n",
    "#### Some interesting choices, they apply to all variables in the data\n",
    "\n",
    "claim_time_occured -> binarized, between 07:00 and 22:00 is day, others AND missing is night\n",
    "\n",
    "all categorical NaN values -> encode as MISSING\n",
    "\n",
    "all numerical NaN values -> encode with extreme outlier, for example vehicle_date_inuse == 1900\n",
    "\n",
    "high cardinality categoricals -> frequency encoding, NaN values get frequency of 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# claim variables\n",
    "\n",
    "train_data['claim_amount'] = train_data['claim_amount'].str.replace(',','.').astype('float64')\n",
    "\n",
    "train_data['claim_date_registered'] = pd.to_datetime(train_data['claim_date_registered'], format='%Y%m%d')\n",
    "\n",
    "train_data['claim_date_occured'] = pd.to_datetime(train_data['claim_date_occured'], format='%Y%m%d')\n",
    "\n",
    "mask_night = (train_data['claim_time_occured'] >= 2200) | (train_data['claim_time_occured'] <= 700)\n",
    "train_data.loc[~mask_night, 'claim_time_occured'] = 0\n",
    "train_data.loc[mask_night, 'claim_time_occured'] = 1\n",
    "\n",
    "postal_code_counts = train_data['claim_postal_code'].value_counts()\n",
    "train_data = train_data.merge(postal_code_counts, how='left', left_on='claim_postal_code', right_index=True)\n",
    "\n",
    "train_data['claim_alcohol'].fillna(\"MISSING\", inplace=True)\n",
    "\n",
    "train_data['claim_language'].fillna(\"MISSING\", inplace=True)\n",
    "mask = train_data['claim_language'] == 1.0\n",
    "train_data.loc[mask, 'claim_language'] = \"LANG A\"\n",
    "mask = train_data['claim_language'] == 2.0\n",
    "train_data.loc[mask, 'claim_language'] = \"LANG B\"\n",
    "\n",
    "train_data['claim_vehicle_id'].fillna(\"MISSING\", inplace=True)\n",
    "claim_vehicle_id_count = train_data['claim_vehicle_id'].value_counts()\n",
    "claim_vehicle_id_count[\"MISSING\"] = 0\n",
    "train_data = train_data.merge(claim_vehicle_id_count, how='left', \n",
    "                              left_on='claim_vehicle_id', right_index=True)\n",
    "\n",
    "train_data['claim_vehicle_brand'].fillna('MISSING', inplace=True)\n",
    "claim_vehicle_brand_counts = train_data['claim_vehicle_brand'].value_counts()\n",
    "claim_vehicle_brand_counts['MISSING'] = 0\n",
    "train_data = train_data.merge(claim_vehicle_brand_counts, how='left', \n",
    "                              left_on='claim_vehicle_brand', right_index=True)\n",
    "\n",
    "\n",
    "train_data['claim_vehicle_type'].fillna('MISSING', inplace=True)\n",
    "\n",
    "train_data['claim_vehicle_date_inuse'].fillna(190001, inplace=True)\n",
    "train_data['claim_vehicle_date_inuse'] = train_data['claim_vehicle_date_inuse'].astype(int)\n",
    "mask = (train_data['claim_vehicle_date_inuse'] > 220000)\n",
    "train_data.loc[mask, 'claim_vehicle_date_inuse'] = 190001\n",
    "train_data['claim_vehicle_date_inuse'] = pd.to_datetime(train_data['claim_vehicle_date_inuse'].astype(str), \n",
    "                                                        format='%Y%m')\n",
    "\n",
    "train_data['claim_vehicle_cyl'].fillna(10000, inplace=True)\n",
    "\n",
    "train_data['claim_vehicle_load'].fillna(500, inplace=True)\n",
    "\n",
    "train_data['claim_vehicle_fuel_type'].fillna('MISSING', inplace=True)\n",
    "mask = train_data['claim_vehicle_fuel_type'] == 1.0\n",
    "train_data.loc[mask, 'claim_vehicle_fuel_type'] = \"FUEL A\"\n",
    "mask = train_data['claim_vehicle_fuel_type'] == 2.0\n",
    "train_data.loc[mask, 'claim_vehicle_fuel_type'] = \"FUEL B\"\n",
    "\n",
    "train_data['claim_vehicle_power'].fillna(1000, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy variables\n",
    "\n",
    "policy_holder_id_count = train_data['policy_holder_id'].value_counts()\n",
    "train_data = train_data.merge(policy_holder_id_count, how='left', \n",
    "                              left_on='policy_holder_id', right_index=True)\n",
    "\n",
    "train_data['policy_holder_postal_code'].fillna(0, inplace=True)\n",
    "policy_holder_postal_code_counts = train_data['policy_holder_postal_code'].value_counts()\n",
    "policy_holder_postal_code_counts.loc[0] = 0\n",
    "train_data = train_data.merge(policy_holder_postal_code_counts, how='left', \n",
    "                              left_on='policy_holder_postal_code', right_index=True)\n",
    "\n",
    "train_data['policy_holder_year_birth'].fillna(1800, inplace=True)\n",
    "\n",
    "train_data['policy_holder_expert_id'].fillna(\"MISSING\", inplace=True)\n",
    "policy_holder_expert_id_count = train_data['policy_holder_expert_id'].value_counts()\n",
    "policy_holder_expert_id_count['MISSING'] = 0\n",
    "train_data = train_data.merge(policy_holder_expert_id_count, how='left', \n",
    "                              left_on='policy_holder_expert_id', right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver variables\n",
    "\n",
    "driver_id_count = train_data['driver_id'].value_counts()\n",
    "train_data = train_data.merge(driver_id_count, how='left', \n",
    "                              left_on='driver_id', right_index=True)\n",
    "\n",
    "train_data['driver_postal_code'].fillna(0, inplace=True)\n",
    "driver_postal_code_count = train_data['driver_postal_code'].value_counts()\n",
    "driver_postal_code_count.loc[0] = 0\n",
    "train_data = train_data.merge(driver_postal_code_count, how='left', \n",
    "                              left_on='driver_postal_code', right_index=True)\n",
    "\n",
    "train_data['driver_year_birth'].fillna(1801, inplace=True)\n",
    "\n",
    "train_data['driver_expert_id'].fillna(\"MISSING\", inplace=True)\n",
    "driver_expert_id_count = train_data['driver_expert_id'].value_counts()\n",
    "driver_expert_id_count['MISSING'] = 0\n",
    "train_data = train_data.merge(driver_expert_id_count, how='left', \n",
    "                              left_on='driver_expert_id', right_index=True)\n",
    "\n",
    "train_data['driver_vehicle_id'].fillna(\"MISSING\", inplace=True)\n",
    "driver_vehicle_id_count = train_data['driver_vehicle_id'].value_counts()\n",
    "driver_vehicle_id_count[\"MISSING\"] = 0\n",
    "train_data = train_data.merge(driver_vehicle_id_count, how='left', \n",
    "                              left_on='driver_vehicle_id', right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# third party 1st variables\n",
    "\n",
    "train_data['third_party_1_id'].fillna(\"MISSING\", inplace=True)\n",
    "third_party_1_id_count = train_data['third_party_1_id'].value_counts()\n",
    "third_party_1_id_count[\"MISSING\"] = 0\n",
    "train_data = train_data.merge(third_party_1_id_count, how='left', \n",
    "                              left_on='third_party_1_id', right_index=True)\n",
    "\n",
    "train_data['third_party_1_postal_code'].fillna(0, inplace=True)\n",
    "third_party_1_postal_code_count = train_data['third_party_1_postal_code'].value_counts()\n",
    "third_party_1_postal_code_count[0] = 0\n",
    "train_data = train_data.merge(third_party_1_postal_code_count, how='left', \n",
    "                              left_on='third_party_1_postal_code', right_index=True)\n",
    "\n",
    "train_data['third_party_1_injured'].fillna(\"MISSING\", inplace=True)\n",
    "\n",
    "train_data['third_party_1_vehicle_type'].fillna(\"MISSING\", inplace=True)\n",
    "\n",
    "train_data['third_party_1_form'].fillna(\"MISSING\", inplace=True)\n",
    "\n",
    "train_data['third_party_1_year_birth'].fillna(1802, inplace=True)\n",
    "\n",
    "train_data['third_party_1_country'].fillna(\"MISSING\", inplace=True)\n",
    "\n",
    "train_data['third_party_1_vehicle_id'].fillna(\"MISSING\", inplace=True)\n",
    "third_party_1_vehicle_id_count = train_data['third_party_1_vehicle_id'].value_counts()\n",
    "third_party_1_vehicle_id_count[\"MISSING\"] = 0\n",
    "train_data = train_data.merge(third_party_1_vehicle_id_count, how='left', \n",
    "                              left_on='third_party_1_vehicle_id', right_index=True)\n",
    "\n",
    "train_data['third_party_1_expert_id'].fillna(\"MISSING\", inplace=True)\n",
    "third_party_1_expert_id_count = train_data['third_party_1_expert_id'].value_counts()\n",
    "third_party_1_expert_id_count[\"MISSING\"] = 0\n",
    "train_data = train_data.merge(third_party_1_expert_id_count, how='left', \n",
    "                 left_on='third_party_1_expert_id', right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3rd party 2nd and 3rd variables\n",
    "\n",
    "These were encoded as simple 0/1 for \"missing\" and \"available\". We believe that they are too far removed from the event and this aggregation should be more than enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# third party rest of variables\n",
    "\n",
    "mask = train_data['third_party_2_id'].isna()\n",
    "train_data.loc[mask, 'third_party_2_id'] = 0\n",
    "train_data.loc[~mask, 'third_party_2_id'] = 1\n",
    "\n",
    "mask = train_data['third_party_2_postal_code'].isna()\n",
    "train_data.loc[mask, 'third_party_2_postal_code'] = 0\n",
    "train_data.loc[~mask, 'third_party_2_postal_code'] = 1\n",
    "\n",
    "mask = train_data['third_party_2_injured'].isna()\n",
    "train_data.loc[mask, 'third_party_2_injured'] = 0\n",
    "train_data.loc[~mask, 'third_party_2_injured'] = 1\n",
    "\n",
    "mask = train_data['third_party_2_vehicle_type'].isna()\n",
    "train_data.loc[mask, 'third_party_2_vehicle_type'] = 0\n",
    "train_data.loc[~mask, 'third_party_2_vehicle_type'] = 1\n",
    "\n",
    "mask = train_data['third_party_2_form'].isna()\n",
    "train_data.loc[mask, 'third_party_2_form'] = 0\n",
    "train_data.loc[~mask, 'third_party_2_form'] = 1\n",
    "\n",
    "mask = train_data['third_party_2_year_birth'].isna()\n",
    "train_data.loc[mask, 'third_party_2_year_birth'] = 0\n",
    "train_data.loc[~mask, 'third_party_2_year_birth'] = 1\n",
    "\n",
    "mask = train_data['third_party_2_country'].isna()\n",
    "train_data.loc[mask, 'third_party_2_country'] = 0\n",
    "train_data.loc[~mask, 'third_party_2_country'] = 1\n",
    "\n",
    "mask = train_data['third_party_2_vehicle_id'].isna()\n",
    "train_data.loc[mask, 'third_party_2_vehicle_id'] = 0\n",
    "train_data.loc[~mask, 'third_party_2_vehicle_id'] = 1\n",
    "\n",
    "mask = train_data['third_party_2_expert_id'].isna()\n",
    "train_data.loc[mask, 'third_party_2_expert_id'] = 0\n",
    "train_data.loc[~mask, 'third_party_2_expert_id'] = 1\n",
    "\n",
    "mask = train_data['third_party_3_id'].isna()\n",
    "train_data.loc[mask, 'third_party_3_id'] = 0\n",
    "train_data.loc[~mask, 'third_party_3_id'] = 1\n",
    "\n",
    "mask = train_data['third_party_3_postal_code'].isna()\n",
    "train_data.loc[mask, 'third_party_3_postal_code'] = 0\n",
    "train_data.loc[~mask, 'third_party_3_postal_code'] = 1\n",
    "\n",
    "mask = train_data['third_party_3_injured'].isna()\n",
    "train_data.loc[mask, 'third_party_3_injured'] = 0\n",
    "train_data.loc[~mask, 'third_party_3_injured'] = 1\n",
    "\n",
    "mask = train_data['third_party_3_vehicle_type'].isna()\n",
    "train_data.loc[mask, 'third_party_3_vehicle_type'] = 0\n",
    "train_data.loc[~mask, 'third_party_3_vehicle_type'] = 1\n",
    "\n",
    "mask = train_data['third_party_3_form'].isna()\n",
    "train_data.loc[mask, 'third_party_3_form'] = 0\n",
    "train_data.loc[~mask, 'third_party_3_form'] = 1\n",
    "\n",
    "mask = train_data['third_party_3_year_birth'].isna()\n",
    "train_data.loc[mask, 'third_party_3_year_birth'] = 0\n",
    "train_data.loc[~mask, 'third_party_3_year_birth'] = 1\n",
    "\n",
    "mask = train_data['third_party_3_country'].isna()\n",
    "train_data.loc[mask, 'third_party_3_country'] = 0\n",
    "train_data.loc[~mask, 'third_party_3_country'] = 1\n",
    "\n",
    "mask = train_data['third_party_3_vehicle_id'].isna()\n",
    "train_data.loc[mask, 'third_party_3_vehicle_id'] = 0\n",
    "train_data.loc[~mask, 'third_party_3_vehicle_id'] = 1\n",
    "\n",
    "mask = train_data['third_party_3_expert_id'].isna()\n",
    "train_data.loc[mask, 'third_party_3_expert_id'] = 0\n",
    "train_data.loc[~mask, 'third_party_3_expert_id'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repair variables\n",
    "\n",
    "train_data['repair_id'].fillna(\"MISSING\", inplace=True)\n",
    "repair_id_count = train_data['repair_id'].value_counts()\n",
    "repair_id_count[\"MISSING\"] = 0\n",
    "train_data = train_data.merge(repair_id_count, how='left', \n",
    "                 left_on='repair_id', right_index=True)\n",
    "\n",
    "train_data['repair_postal_code'].fillna(0, inplace=True)\n",
    "repair_postal_code_count = train_data['repair_postal_code'].value_counts()\n",
    "repair_postal_code_count[0] = 0\n",
    "train_data = train_data.merge(repair_postal_code_count, how='left', \n",
    "                 left_on='repair_postal_code', right_index=True)\n",
    "\n",
    "train_data['repair_form'].fillna(\"MISSING\", inplace=True)\n",
    "\n",
    "train_data['repair_year_birth'].fillna(1804, inplace=True)\n",
    "\n",
    "train_data['repair_country'].fillna(\"MISSING\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final variables\n",
    "\n",
    "train_data['policy_date_start'].fillna(180501, inplace=True)\n",
    "train_data['policy_date_start'] = pd.to_datetime(train_data['policy_date_start'].astype(int).astype(str),\n",
    "                                                 format='%Y%m')\n",
    "\n",
    "train_data['policy_date_next_expiry'].fillna(180501, inplace=True)\n",
    "train_data['policy_date_next_expiry'] = pd.to_datetime(train_data['policy_date_next_expiry'].astype(int).astype(str),\n",
    "                                                       format='%Y%m')\n",
    "\n",
    "train_data['policy_date_last_renewed'].fillna(180501, inplace=True)\n",
    "train_data['policy_date_last_renewed'] = pd.to_datetime(train_data['policy_date_last_renewed'].astype(int).astype(str),\n",
    "                                                        format='%Y%m')\n",
    "\n",
    "train_data['policy_premium_100'].fillna(200, inplace=True)\n",
    "\n",
    "train_data['policy_coverage_1000'].fillna(300, inplace=True)\n",
    "\n",
    "train_data['policy_coverage_type'].fillna(\"MISSING\", inplace=True)\n",
    "policy_coverage_type_count = train_data['policy_coverage_type'].value_counts()\n",
    "policy_coverage_type_count[\"MISSING\"] = 0\n",
    "train_data = train_data.merge(policy_coverage_type_count, how='left', \n",
    "                 left_on='policy_coverage_type', right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal data\n",
    "\n",
    "Temporal data was featurized into Month, Day, DayOfWeek, (Date claimed - Date Happened), DaysLeftInContract, DaysSinceContract, etc.\n",
    "\n",
    "However, validation later showed that these variables make the model worse as they mostly correlate with claim amounts of small value. Thus, they were left out of the model."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# time data and time features\n",
    "\n",
    "#train_data['claim_registered_month'] = train_data.claim_date_registered.dt.month\n",
    "#train_data['claim_registered_day'] = train_data.claim_date_registered.dt.day\n",
    "#train_data['claim_registered_weekday'] = train_data.claim_date_registered.dt.dayofweek\n",
    "\n",
    "train_data['claim_late'] = (train_data.claim_date_registered - train_data.claim_date_occured).dt.days\n",
    "#train_data['claim_occured_weekday'] = train_data.claim_date_occured.dt.dayofweek\n",
    "\n",
    "#train_data['vehicle_year'] = train_data.claim_vehicle_date_inuse.dt.year\n",
    "\n",
    "train_data['days_left'] = (train_data.policy_date_next_expiry - train_data.claim_date_occured).dt.days\n",
    "train_data['days_since'] = (train_data.claim_date_occured - train_data.policy_date_start).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "third_cols = train_data.filter(regex='third_party_2|third_party_3').columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We now do the same for the validation data. When performing frequency encoding, we use the frequencies calculated with the training data. This code could probably be optimized and modularized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# claim variables\n",
    "\n",
    "#train_data['claim_amount'] = train_data['claim_amount'].str.replace(',','.').astype('float64')\n",
    "\n",
    "test_data['claim_date_registered'] = pd.to_datetime(test_data['claim_date_registered'], format='%Y%m%d')\n",
    "\n",
    "test_data['claim_date_occured'] = pd.to_datetime(test_data['claim_date_occured'], format='%Y%m%d')\n",
    "\n",
    "mask_night = (test_data['claim_time_occured'] >= 2200) | (test_data['claim_time_occured'] <= 700)\n",
    "test_data.loc[~mask_night, 'claim_time_occured'] = 0\n",
    "test_data.loc[mask_night, 'claim_time_occured'] = 1\n",
    "\n",
    "# postal_code_counts = train_data['claim_postal_code'].value_counts()\n",
    "test_data = test_data.merge(postal_code_counts, how='left', left_on='claim_postal_code', right_index=True)\n",
    "\n",
    "test_data['claim_alcohol'].fillna(\"MISSING\", inplace=True)\n",
    "\n",
    "test_data['claim_language'].fillna(\"MISSING\", inplace=True)\n",
    "mask = test_data['claim_language'] == 1.0\n",
    "test_data.loc[mask, 'claim_language'] = \"LANG A\"\n",
    "mask = test_data['claim_language'] == 2.0\n",
    "test_data.loc[mask, 'claim_language'] = \"LANG B\"\n",
    "\n",
    "test_data['claim_vehicle_id'].fillna(\"MISSING\", inplace=True)\n",
    "#claim_vehicle_id_count = train_data['claim_vehicle_id'].value_counts()\n",
    "#claim_vehicle_id_count[\"MISSING\"] = 0\n",
    "test_data = test_data.merge(claim_vehicle_id_count, how='left', \n",
    "                              left_on='claim_vehicle_id', right_index=True)\n",
    "\n",
    "test_data['claim_vehicle_brand'].fillna('MISSING', inplace=True)\n",
    "#claim_vehicle_brand_counts = train_data['claim_vehicle_brand'].value_counts()\n",
    "#claim_vehicle_brand_counts['MISSING'] = 0\n",
    "test_data = test_data.merge(claim_vehicle_brand_counts, how='left', \n",
    "                              left_on='claim_vehicle_brand', right_index=True)\n",
    "\n",
    "test_data['claim_vehicle_date_inuse'].fillna(190001.0, inplace=True)\n",
    "mask = (test_data['claim_vehicle_date_inuse'] > 210001.0)\n",
    "test_data.loc[mask, 'claim_vehicle_date_inuse'] = 190001.0\n",
    "test_data['claim_vehicle_date_inuse'] = pd.to_datetime(test_data['claim_vehicle_date_inuse'].astype(int).astype(str), \n",
    "                                                        format='%Y%m')\n",
    "\n",
    "test_data['claim_vehicle_cyl'].fillna(10000, inplace=True)\n",
    "\n",
    "test_data['claim_vehicle_load'].fillna(500, inplace=True)\n",
    "\n",
    "test_data['claim_vehicle_fuel_type'].fillna('MISSING', inplace=True)\n",
    "mask = test_data['claim_vehicle_fuel_type'] == 1.0\n",
    "test_data.loc[mask, 'claim_vehicle_fuel_type'] = \"FUEL A\"\n",
    "mask = test_data['claim_vehicle_fuel_type'] == 2.0\n",
    "test_data.loc[mask, 'claim_vehicle_fuel_type'] = \"FUEL B\"\n",
    "\n",
    "test_data['claim_vehicle_type'].fillna('MISSING', inplace=True)\n",
    "\n",
    "test_data['claim_vehicle_power'].fillna(1000, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# policy variables\n",
    "\n",
    "#policy_holder_id_count = train_data['policy_holder_id'].value_counts()\n",
    "test_data = test_data.merge(policy_holder_id_count, how='left', \n",
    "                              left_on='policy_holder_id', right_index=True)\n",
    "\n",
    "test_data['policy_holder_postal_code'].fillna(0, inplace=True)\n",
    "#policy_holder_postal_code_counts = train_data['policy_holder_postal_code'].value_counts()\n",
    "#policy_holder_postal_code_counts.loc[0] = 0\n",
    "test_data = test_data.merge(policy_holder_postal_code_counts, how='left', \n",
    "                              left_on='policy_holder_postal_code', right_index=True)\n",
    "\n",
    "test_data['policy_holder_year_birth'].fillna(1800, inplace=True)\n",
    "\n",
    "test_data['policy_holder_expert_id'].fillna(\"MISSING\", inplace=True)\n",
    "#policy_holder_expert_id_count = train_data['policy_holder_expert_id'].value_counts()\n",
    "#policy_holder_expert_id_count['MISSING'] = 0\n",
    "test_data = test_data.merge(policy_holder_expert_id_count, how='left', \n",
    "                              left_on='policy_holder_expert_id', right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# driver variables\n",
    "\n",
    "#driver_id_count = train_data['driver_id'].value_counts()\n",
    "test_data = test_data.merge(driver_id_count, how='left', \n",
    "                              left_on='driver_id', right_index=True)\n",
    "\n",
    "test_data['driver_postal_code'].fillna(0, inplace=True)\n",
    "#driver_postal_code_count = train_data['driver_postal_code'].value_counts()\n",
    "#driver_postal_code_count.loc[0] = 0\n",
    "test_data = test_data.merge(driver_postal_code_count, how='left', \n",
    "                              left_on='driver_postal_code', right_index=True)\n",
    "\n",
    "test_data['driver_year_birth'].fillna(1801, inplace=True)\n",
    "\n",
    "test_data['driver_expert_id'].fillna(\"MISSING\", inplace=True)\n",
    "#driver_expert_id_count = train_data['driver_expert_id'].value_counts()\n",
    "#driver_expert_id_count['MISSING'] = 0\n",
    "test_data = test_data.merge(driver_expert_id_count, how='left', \n",
    "                              left_on='driver_expert_id', right_index=True)\n",
    "\n",
    "test_data['driver_vehicle_id'].fillna(\"MISSING\", inplace=True)\n",
    "#driver_vehicle_id_count = train_data['driver_vehicle_id'].value_counts()\n",
    "#driver_vehicle_id_count[\"MISSING\"] = 0\n",
    "test_data = test_data.merge(driver_vehicle_id_count, how='left', \n",
    "                              left_on='driver_vehicle_id', right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# third party 1st variables\n",
    "\n",
    "test_data['third_party_1_id'].fillna(\"MISSING\", inplace=True)\n",
    "#third_party_1_id_count = train_data['third_party_1_id'].value_counts()\n",
    "#third_party_1_id_count[\"MISSING\"] = 0\n",
    "test_data = test_data.merge(third_party_1_id_count, how='left', \n",
    "                              left_on='third_party_1_id', right_index=True)\n",
    "\n",
    "test_data['third_party_1_postal_code'].fillna(0, inplace=True)\n",
    "#third_party_1_postal_code_count = train_data['third_party_1_postal_code'].value_counts()\n",
    "#third_party_1_postal_code_count[0] = 0\n",
    "test_data = test_data.merge(third_party_1_postal_code_count, how='left', \n",
    "                              left_on='third_party_1_postal_code', right_index=True)\n",
    "\n",
    "test_data['third_party_1_injured'].fillna(\"MISSING\", inplace=True)\n",
    "\n",
    "test_data['third_party_1_vehicle_type'].fillna(\"MISSING\", inplace=True)\n",
    "\n",
    "test_data['third_party_1_form'].fillna(\"MISSING\", inplace=True)\n",
    "\n",
    "test_data['third_party_1_year_birth'].fillna(1802, inplace=True)\n",
    "\n",
    "test_data['third_party_1_country'].fillna(\"MISSING\", inplace=True)\n",
    "\n",
    "test_data['third_party_1_vehicle_id'].fillna(\"MISSING\", inplace=True)\n",
    "#third_party_1_vehicle_id_count = train_data['third_party_1_vehicle_id'].value_counts()\n",
    "#third_party_1_vehicle_id_count[\"MISSING\"] = 0\n",
    "test_data = test_data.merge(third_party_1_vehicle_id_count, how='left', \n",
    "                              left_on='third_party_1_vehicle_id', right_index=True)\n",
    "\n",
    "test_data['third_party_1_expert_id'].fillna(\"MISSING\", inplace=True)\n",
    "#third_party_1_expert_id_count = train_data['third_party_1_expert_id'].value_counts()\n",
    "#third_party_1_expert_id_count[\"MISSING\"] = 0\n",
    "test_data = test_data.merge(third_party_1_expert_id_count, how='left', \n",
    "                 left_on='third_party_1_expert_id', right_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# third party rest of variables\n",
    "\n",
    "mask = test_data['third_party_2_id'].isna()\n",
    "test_data.loc[mask, 'third_party_2_id'] = 0\n",
    "test_data.loc[~mask, 'third_party_2_id'] = 1\n",
    "\n",
    "mask = test_data['third_party_2_postal_code'].isna()\n",
    "test_data.loc[mask, 'third_party_2_postal_code'] = 0\n",
    "test_data.loc[~mask, 'third_party_2_postal_code'] = 1\n",
    "\n",
    "mask = test_data['third_party_2_injured'].isna()\n",
    "test_data.loc[mask, 'third_party_2_injured'] = 0\n",
    "test_data.loc[~mask, 'third_party_2_injured'] = 1\n",
    "\n",
    "mask = test_data['third_party_2_vehicle_type'].isna()\n",
    "test_data.loc[mask, 'third_party_2_vehicle_type'] = 0\n",
    "test_data.loc[~mask, 'third_party_2_vehicle_type'] = 1\n",
    "\n",
    "mask = test_data['third_party_2_form'].isna()\n",
    "test_data.loc[mask, 'third_party_2_form'] = 0\n",
    "test_data.loc[~mask, 'third_party_2_form'] = 1\n",
    "\n",
    "mask = test_data['third_party_2_year_birth'].isna()\n",
    "test_data.loc[mask, 'third_party_2_year_birth'] = 0\n",
    "test_data.loc[~mask, 'third_party_2_year_birth'] = 1\n",
    "\n",
    "mask = test_data['third_party_2_country'].isna()\n",
    "test_data.loc[mask, 'third_party_2_country'] = 0\n",
    "test_data.loc[~mask, 'third_party_2_country'] = 1\n",
    "\n",
    "mask = test_data['third_party_2_vehicle_id'].isna()\n",
    "test_data.loc[mask, 'third_party_2_vehicle_id'] = 0\n",
    "test_data.loc[~mask, 'third_party_2_vehicle_id'] = 1\n",
    "\n",
    "mask = test_data['third_party_2_expert_id'].isna()\n",
    "test_data.loc[mask, 'third_party_2_expert_id'] = 0\n",
    "test_data.loc[~mask, 'third_party_2_expert_id'] = 1\n",
    "\n",
    "mask = test_data['third_party_3_id'].isna()\n",
    "test_data.loc[mask, 'third_party_3_id'] = 0\n",
    "test_data.loc[~mask, 'third_party_3_id'] = 1\n",
    "\n",
    "mask = test_data['third_party_3_postal_code'].isna()\n",
    "test_data.loc[mask, 'third_party_3_postal_code'] = 0\n",
    "test_data.loc[~mask, 'third_party_3_postal_code'] = 1\n",
    "\n",
    "mask = test_data['third_party_3_injured'].isna()\n",
    "test_data.loc[mask, 'third_party_3_injured'] = 0\n",
    "test_data.loc[~mask, 'third_party_3_injured'] = 1\n",
    "\n",
    "mask = test_data['third_party_3_vehicle_type'].isna()\n",
    "test_data.loc[mask, 'third_party_3_vehicle_type'] = 0\n",
    "test_data.loc[~mask, 'third_party_3_vehicle_type'] = 1\n",
    "\n",
    "mask = test_data['third_party_3_form'].isna()\n",
    "test_data.loc[mask, 'third_party_3_form'] = 0\n",
    "test_data.loc[~mask, 'third_party_3_form'] = 1\n",
    "\n",
    "mask = test_data['third_party_3_year_birth'].isna()\n",
    "test_data.loc[mask, 'third_party_3_year_birth'] = 0\n",
    "test_data.loc[~mask, 'third_party_3_year_birth'] = 1\n",
    "\n",
    "mask = test_data['third_party_3_country'].isna()\n",
    "test_data.loc[mask, 'third_party_3_country'] = 0\n",
    "test_data.loc[~mask, 'third_party_3_country'] = 1\n",
    "\n",
    "mask = test_data['third_party_3_vehicle_id'].isna()\n",
    "test_data.loc[mask, 'third_party_3_vehicle_id'] = 0\n",
    "test_data.loc[~mask, 'third_party_3_vehicle_id'] = 1\n",
    "\n",
    "mask = test_data['third_party_3_expert_id'].isna()\n",
    "test_data.loc[mask, 'third_party_3_expert_id'] = 0\n",
    "test_data.loc[~mask, 'third_party_3_expert_id'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repair variables\n",
    "\n",
    "test_data['repair_id'].fillna(\"MISSING\", inplace=True)\n",
    "#repair_id_count = train_data['repair_id'].value_counts()\n",
    "#repair_id_count[\"MISSING\"] = 0\n",
    "test_data = test_data.merge(repair_id_count, how='left', \n",
    "                 left_on='repair_id', right_index=True)\n",
    "\n",
    "test_data['repair_postal_code'].fillna(0, inplace=True)\n",
    "#repair_postal_code_count = train_data['repair_postal_code'].value_counts()\n",
    "#repair_postal_code_count[0] = 0\n",
    "test_data = test_data.merge(repair_postal_code_count, how='left', \n",
    "                 left_on='repair_postal_code', right_index=True)\n",
    "\n",
    "test_data['repair_form'].fillna(\"MISSING\", inplace=True)\n",
    "\n",
    "test_data['repair_year_birth'].fillna(1804, inplace=True)\n",
    "\n",
    "test_data['repair_country'].fillna(\"MISSING\", inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final variables\n",
    "\n",
    "test_data['policy_date_start'].fillna(180501, inplace=True)\n",
    "test_data['policy_date_start'] = pd.to_datetime(test_data['policy_date_start'].astype(int).astype(str),\n",
    "                                                 format='%Y%m')\n",
    "\n",
    "test_data['policy_date_next_expiry'].fillna(180501, inplace=True)\n",
    "test_data['policy_date_next_expiry'] = pd.to_datetime(test_data['policy_date_next_expiry'].astype(int).astype(str),\n",
    "                                                       format='%Y%m')\n",
    "\n",
    "test_data['policy_date_last_renewed'].fillna(180501, inplace=True)\n",
    "test_data['policy_date_last_renewed'] = pd.to_datetime(test_data['policy_date_last_renewed'].astype(int).astype(str),\n",
    "                                                        format='%Y%m')\n",
    "\n",
    "test_data['policy_premium_100'].fillna(200, inplace=True)\n",
    "\n",
    "test_data['policy_coverage_1000'].fillna(300, inplace=True)\n",
    "\n",
    "test_data['policy_coverage_type'].fillna(\"MISSING\", inplace=True)\n",
    "#policy_coverage_type_count = train_data['policy_coverage_type'].value_counts()\n",
    "#policy_coverage_type_count[\"MISSING\"] = 0\n",
    "test_data = test_data.merge(policy_coverage_type_count, how='left', \n",
    "                 left_on='policy_coverage_type', right_index=True)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# time data and time features\n",
    "\n",
    "#test_data['claim_registered_month'] = test_data.claim_date_registered.dt.month\n",
    "#test_data['claim_registered_day'] = test_data.claim_date_registered.dt.day\n",
    "#test_data['claim_registered_weekday'] = test_data.claim_date_registered.dt.dayofweek\n",
    "\n",
    "test_data['claim_late'] = (test_data.claim_date_registered - test_data.claim_date_occured).dt.days\n",
    "#test_data['claim_occured_weekday'] = test_data.claim_date_occured.dt.dayofweek\n",
    "\n",
    "#test_data['vehicle_year'] = test_data.claim_vehicle_date_inuse.dt.year\n",
    "\n",
    "test_data['days_left'] = (test_data.policy_date_next_expiry - test_data.claim_date_occured).dt.days\n",
    "test_data['days_since'] = (test_data.claim_date_occured - test_data.policy_date_start).dt.days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some missing values in the validation data, new ones that were not handled by the pipeline, are assigned as 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop the original columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop cols for now\n",
    "dropcols = ['claim_id', 'claim_postal_code_x', 'claim_vehicle_id_x', 'claim_vehicle_brand_x',\n",
    "       'policy_holder_id_x', 'policy_holder_postal_code_x',\n",
    "       'policy_holder_expert_id_x', 'driver_id_x', 'driver_postal_code_x',\n",
    "       'driver_expert_id_x', 'driver_vehicle_id_x', 'third_party_1_id_x',\n",
    "       'third_party_1_postal_code_x', 'third_party_1_vehicle_id_x',\n",
    "       'third_party_1_expert_id_x', 'repair_id_x', 'repair_postal_code_x',\n",
    "       'claim_date_registered', 'claim_date_occured', 'claim_vehicle_date_inuse',\n",
    "       'policy_date_start', 'policy_date_next_expiry', 'policy_date_last_renewed', 'policy_coverage_type_x']\n",
    "\n",
    "clean_data = train_data.drop(columns=dropcols)\n",
    "\n",
    "# encode predictors\n",
    "test_data['claim_amount'] = test_data['claim_amount'].str.replace(',','.').astype('float64')\n",
    "X_test = test_data.drop(columns=dropcols).drop(columns=['fraud', 'claim_amount'])\n",
    "encoded_predictors = pd.get_dummies(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The strategy\n",
    "\n",
    "Our strategy is to train a Random Forest on the data. The goal is to maximize the claim amount in the 100 most probable claims, as classified by the model. In order to do so, we can upsample the high value claims, or downsample the low value cases. \n",
    "\n",
    "Here, we remove the entries with a claim amount below some value and proceed to train the model accordingly. Since we are constrained in looking at only 100 cases, we really do NOT want to find claims with 500 euros, since there exist some cases with claim amounts of 15.000 euros, which are 30 times more valueable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random forest score:  0.6251959705168381\n"
     ]
    }
   ],
   "source": [
    "# find all positives with less than 8000E claim amount\n",
    "indices = clean_data[(clean_data['fraud'] == 'Y') & (clean_data['claim_amount'] < 8000)].index\n",
    "#print(indices)\n",
    "\n",
    "# target and covariates\n",
    "# drop tiny amount positives\n",
    "y_train = clean_data.drop(indices)['fraud']\n",
    "X_train = clean_data.drop(indices).drop(columns=['fraud', 'claim_amount'])\n",
    "\n",
    "# cast correct dtype - this is just needed, no smart thing happening\n",
    "X_train[third_cols] = X_train[third_cols].apply(pd.to_numeric)\n",
    "\n",
    "# encode covars\n",
    "encoded_covariates = pd.get_dummies(X_train)\n",
    "encoded_target = pd.get_dummies(y_train)['Y']\n",
    "\n",
    "######################\n",
    "### run model ver ####\n",
    "######################\n",
    "\n",
    "# run model a\n",
    "rf = RandomForestClassifier(n_estimators = 200, random_state=10)\n",
    "\n",
    "# predict\n",
    "rf.fit(encoded_covariates, encoded_target)\n",
    "\n",
    "predictions = rf.predict_proba(encoded_predictors)\n",
    "pred_df = pd.DataFrame(predictions)\n",
    "pred_df['claim_id'] = test_data['claim_id'].values\n",
    "pred_df.drop(columns=0, inplace=True)\n",
    "pred_df.sort_values(by=1, ascending=False, inplace=True)\n",
    "\n",
    "# find true positives\n",
    "mask = (test_data['fraud'] == 'Y')\n",
    "#keep amount and ID\n",
    "frauds = test_data.loc[mask, ['claim_id', 'claim_amount']]\n",
    "# merge true positives with predictions and get top 100 predictions, if they are correct, and what we got out\n",
    "score = frauds.merge(pred_df.iloc[:100,:], how='right', on='claim_id')['claim_amount'].sum() / frauds.merge(pred_df, how='right', on='claim_id')['claim_amount'].sum()\n",
    "print('Random forest score: ', score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another model\n",
    "\n",
    "We also try a Gradient Boosting classifier. However, the Random Forest appeared to be more robust and easier to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val:  500 GB score:  0.47201892837636733\n"
     ]
    }
   ],
   "source": [
    "for val in [500, 2000, 4000, 6000, 8000, 10000, 12000]:\n",
    "    # find all positives with less than 1000E claim amount\n",
    "    indices = clean_data[(clean_data['fraud'] == 'Y') & (clean_data['claim_amount'] < val)].index\n",
    "    #print(indices)\n",
    "\n",
    "    # target and covariates\n",
    "    # drop tiny amount positives\n",
    "    y_train = clean_data.drop(indices)['fraud']\n",
    "    X_train = clean_data.drop(indices).drop(columns=['fraud', 'claim_amount'])\n",
    "\n",
    "    # cast correct dtype - this is just needed, no smart thing happening\n",
    "    X_train[third_cols] = X_train[third_cols].apply(pd.to_numeric)\n",
    "\n",
    "    # encode covars\n",
    "    encoded_covariates = pd.get_dummies(X_train)\n",
    "    encoded_target = pd.get_dummies(y_train)['Y']\n",
    "\n",
    "\n",
    "    # run model b\n",
    "    from sklearn.ensemble import GradientBoostingClassifier\n",
    "    from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "    gbc = GradientBoostingClassifier(n_estimators = 90, learning_rate = 0.01, max_depth=4, random_state=10)\n",
    "\n",
    "    # predict\n",
    "    gbc.fit(encoded_covariates, encoded_target)\n",
    "\n",
    "    predictions = gbc.predict_proba(encoded_predictors)\n",
    "    pred_df = pd.DataFrame(predictions)\n",
    "    pred_df['claim_id'] = test_data['claim_id'].values\n",
    "    pred_df.drop(columns=0, inplace=True)\n",
    "    pred_df.sort_values(by=1, ascending=False, inplace=True)\n",
    "\n",
    "    # find true positives\n",
    "    mask = (test_data['fraud'] == 'Y')\n",
    "    #keep amount and ID\n",
    "    frauds = test_data.loc[mask, ['claim_id', 'claim_amount']]\n",
    "    # merge true positives with predictions and get top 100 predictions, if they are correct, and what we got out\n",
    "    score = frauds.merge(pred_df.iloc[:100,:], how='right', on='claim_id')['claim_amount'].sum() / frauds.merge(pred_df, how='right', on='claim_id')['claim_amount'].sum()\n",
    "    print('val: ', val, 'GB score: ', score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc.best_params_, gbc.best_score_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
